ğŸ§  SHL GenAI Assessment Recommendation System

A Retrieval-Augmented Generation (RAG)â€“based AI system that recommends the most relevant SHL assessments for a given user query.
It combines web scraping, vector search, and LangChain-powered retrieval to provide smart, context-aware recommendations.

ğŸš€ Features

âœ… Web Scraping of SHLâ€™s official product catalog (32 pages of â€œIndividual Test Solutionsâ€)
âœ… Cleaned and structured data with detailed metadata like:

Assessment name

Test type (A/K/P/S)

Adaptive / Remote testing

Description, Duration, and Job levels

âœ… RAG pipeline using:

LangChain + ChromaDB for vector storage

Hugging Face Embeddings for semantic search

âœ… Evaluation using Recall@10 metric on labeled queries
âœ… Optimized chunking and retriever logic to improve accuracy
âœ… FastAPI backend + Streamlit frontend for easy interaction
âœ… Render deployment with Hugging Face Inference API (no GPU or torch dependency)

ğŸ§© Tech Stack
Category	Tools / Frameworks
Backend	FastAPI
Frontend	Streamlit
Vector Store	ChromaDB
Embeddings	Hugging Face Inference API (paraphrase-MiniLM-L3-v2)
Language Framework	LangChain
Deployment	Render (free tier)
Scraping	BeautifulSoup
Evaluation	Recall@10 metric
âš™ï¸ System Architecture
User Query
   â†“
Hugging Face Inference API (Embeddings)
   â†“
Chroma Vector Database
   â†“
Retriever (MMR search)
   â†“
Recommended SHL Assessments

ğŸ“Š Project Workflow
1ï¸âƒ£ Web Scraping

Parsed the SHL Product Catalog using BeautifulSoup.

Filtered only â€œIndividual Test Solutions.â€

Extracted key fields (name, URL, test type, duration, etc.)

Saved clean dataset â†’ data/shl_scraped_catalog.csv

2ï¸âƒ£ Data Cleaning

Fixed /solutions/ URL mismatch between training data and scraped data.

Ensured all URLs are standardized for evaluation consistency.

3ï¸âƒ£ RAG Setup

Converted records into LangChain Document objects.

Stored embeddings in ChromaDB.

Used MMR-based retriever for diverse, relevant results.

4ï¸âƒ£ Model Optimization

Initial chunk size: 800 / 100 â†’ Recall@10 = 0.293

Optimized chunk size: 1500 / 200 â†’ Recall@10 = 0.433

Improved retrieval quality by increasing fetch_k to 30 and applying URL normalization.

5ï¸âƒ£ Deployment

Backend API built in FastAPI (/recommend endpoint).

Streamlit app created for user queries and displaying ranked results.

Deployed on Render using Hugging Faceâ€™s hosted inference API to avoid OOM issues.

ğŸ“ˆ Results Summary
Metric	Before	After
Chunk Size / Overlap	800 / 100	1500 / 200
Mean Recall@10	0.293	0.433
URL Consistency	Mismatched	âœ… Fixed
Memory Usage	High (torch model)	âœ… Optimized (HF API)
Runtime	Slow	âœ… Stable on Render
ğŸ§  Key Learnings

Importance of chunk size tuning in embedding-based retrieval

Using MMR retrieval to balance relevance and diversity

Handling data normalization issues (URL mismatches)

Optimizing model choice for low-memory environments like Render

Deploying end-to-end GenAI apps using LangChain + FastAPI + Hugging Face

ğŸ› ï¸ Setup Instructions
ğŸ”§ Prerequisites

Python 3.10 or 3.11

Hugging Face account (for API token)

ğŸ“¦ Installation
git clone https://github.com/ARUN-CREATOR-SVG/SHL-GenAI-Assessment-Recommendation.git
cd SHL-GenAI-Assessment-Recommendation
pip install -r requirements.txt

âš™ï¸ Environment Variables

Create a .env file:

HUGGINGFACEHUB_API_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxx

â–¶ï¸ Run Locally

Start FastAPI backend:

uvicorn app.app:app --reload


Run Streamlit frontend:

streamlit run streamlit_app.py

ğŸŒ Deployment (Render)

Remove heavy dependencies (torch, sentence-transformers)

Use HuggingFaceHubEmbeddings for remote inference

Add your token in Render environment variables

Start command:

python -m uvicorn app.app:app --host 0.0.0.0 --port $PORT


ğŸ Final Output

Fast and accurate recommendations for SHL assessments

Improved Recall@10 = 0.433

Deployed, scalable, and cost-free GenAI system

âœ¨ Author

Arun Singh
ğŸ’¼ AI/ML & Data Science Enthusiast
ğŸ”—[ LinkedIn Profile](https://www.linkedin.com/in/arun-singh-7a7b9b289/)

ğŸ“§[ arunsingh@example.com](arunsin2212@gmail.com)